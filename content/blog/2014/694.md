+++
title = "Hadoop2.3.0源码编译过程记录"
date = "2014-03-28T15:11:49+08:00"
tags = ["storm"]
categories = ["storm"]
banner = "img/banners/banner-2.jpg"
draft = false
author = "helight"
authorlink = "https://helight.cn"
summary = ""
keywords = ["storm"]
+++

    要在hadoop上作一个mr程序，做一些统计工作。但是发现官网下载的2.3的库只有32位的，而我的机器系统都是64位的。用g++直接指定编译32位的程序又发现头文件依赖有问题，没法编译通过。所以就只能编译hadoop，编译后在服务上启动64位程序，再编译mr程序进行工作。下面是编译的过程和其中遇到的一些问题。
<!--more-->
编译文件准备：

protobuf-2.5.0.tar.gz

findbugs-2.0.3-source.zip

gcc, build-essential, libssl-dev, zlib1g-dev, libglib2.0-dev, cmake, maven 这些直接使用apt安装即可，其他两个需要源码安装。

apt-get install gcc build-essential libssl-dev zlib1g-dev libglib2.0-dev cmake maven

Findbugs不安装会报下面的错误：

${env.FINDBUGS_HOME}/src/xsl/default.xsl doesn't exist

findbugs解压之后直接在其源代码目录中执行ant进行编译即可，编译后不需要安装，只需要在环境变量中指定其根目录。

helight:hadoop-2.3.0-src$ vim ~/.bashrc

export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64/

export FINDBUGS_HOME=/data1/tools/findbugs-2.0.3/

当然ant编译要指定jvm的home路径，见上面的配置。



Protobuf必须是2.5.0的版本，低版本编译hadoop会编译不过去。Protobuf在编译安装之后，直接使用回报下面的错误：

helight:hadoop-2.3.0-src$ protoc

protoc: error while loading shared libraries: libprotobuf.so.8: cannot open shared object file: No such file or directory

helight:hadoop-2.3.0-src$

需要在.bashrc中添加：

export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/

这样就可以编译了。

编译命令：

mvn package -Pdist,native,docs -DskipTests -Dtar

使用mvn编译的时候需要联网，它会下载许多相关的类库和配置之类的文件。编译完成之后就可以直接启动了。



接下来就需要编译我的mr程序了，使用c++写的，使用hadoop的pipes模式。我使用scons来编译的：

env = Environment()

env.Append(CCFLAGS = ['-g','-O3'])

env.Append(LIBS = ['hadooppipes','hadooputils', 'pthread', 'crypto', 'z', 'dl'])

env.Append(LIBPATH = ['/data1/hadoop/hadoop/lib/native/'])

env.Append(CPPPATH = ['/data1/hadoop/hadoop/include/'])

env.Program(

target = 'mr_test',

source = ['mr_test.cc'],

)

需要同时链接'hadooppipes','hadooputils', 'pthread', 'crypto', 'z', 'dl'这几个库。

运行mr程序：

pathconf="-conf ./job_config.xml -input /home/test/input/* -output /home/test/output1/"

hadoop pipes $pathconf -program /home/test/mr_test

这里的input和output，还有program目录都是hadoop的hdfs中的目录。output目录是不用人工建立的，在执行mr程序的时候，程序会自动建立，如果人工建立了，执行会报错。

下面是编译过程中遇到的其它一些错误和解决方式：

undefined reference to `BIO_ctrl'

hadoop-2.3.0-src/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc:446: undefined reference to `BIO_free_all'

collect2: error: ld returned 1 exit status

编译的时候加个参数-lcrypto

: In function `bio_zlib_read':

(.text+0xa8d): undefined reference to `zError'

/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/libcrypto.a(c_zlib.o): In function `bio_zlib_read':

(.text+0xafc): undefined reference to `inflateInit_'

编译的时候加个参数-lz

undefined reference to `dlerror'

/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/libcrypto.a(dso_dlfcn.o): In function `dlfcn_unload':

(.text+0x742): undefined reference to `dlclose'

编译的时候加个参数-ldl

Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.7:run (make) on project hadoop-common: An Ant BuildException has occured: Execute failed: java.io.IOException: Cannot run program "cmake" (in directory "/data1/hadoop-2.3.0-src/hadoop-common-project/hadoop-common/target/native"): j

cmake没有安装

configure: error: C++ preprocessor "/lib/cpp" fails sanity check
安装build-essential

Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (make) on project hadoop-hdfs: An Ant BuildException has occured: exec returned: 1 -> [Help 1]
安装libglib2.0-dev

<center>
看完本文有收获？请分享给更多人<br>

关注「黑光技术」，关注大数据+微服务<br>

![](/img/qrcode_helight_tech.jpg)
</center>
